{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单执行\n",
    "\n",
    "构建一个执行各类强化学习算法的流水线。\n",
    "\n",
    "目前执行三种算法：DQN（value-based近似q函数），DDPG（actor-critic），PPO（用的比较多）。\n",
    "\n",
    "## 环境的描述\n",
    "\n",
    "考虑一个简单的例子（Pendulum：单摆）\n",
    "\n",
    "动作空间：尾部受力，一维连续空间\n",
    "观测空间：尾部位置二维坐标，角速度，三维连续空间\n",
    "奖励：r = -(theta^2 + 0.1 * theta_dt^2 + 0.001 * torque^2)\n",
    "\n",
    "一个episode执行到200步发生truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, SupportsFloat\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class PendulumEnv(gym.Wrapper):\n",
    "    def __init__(self, gym_id=None):\n",
    "        gym.logger.set_level(40)\n",
    "        if gym_id is None:\n",
    "            gym_id = \"Pendulum-v1\"\n",
    "        super().__init__(env=gym.make(gym_id))\n",
    "        self.id = gym_id\n",
    "        self.state_dim = self.observation_space.shape[0]\n",
    "        self.action_dim = self.action_space.shape[0]\n",
    "        self.if_discrete = False # 是否是离散动作空间\n",
    "    def reset(self) -> \"tuple[Any, dict[str, Any]]\":\n",
    "        return self.env.reset()\n",
    "    def step(self, action: Any) -> \"tuple[Any, SupportsFloat, bool, bool, dict[str, Any]]\":\n",
    "        # OpenAI Pendulum env set its action space as (-2, +2). It is bad.\n",
    "        # We suggest that adjust action space to (-1, +1) when designing a custom env.\n",
    "        state, reward, terminated, truncated, info = self.env.step(action * 2)\n",
    "        state = state.reshape(self.state_dim)\n",
    "        return state, float(reward), terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试Env类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Finish checking.\n"
     ]
    }
   ],
   "source": [
    "def check_pendulum_env():\n",
    "    env = PendulumEnv()\n",
    "    assert isinstance(env.id, str)\n",
    "    assert isinstance(env.state_dim, int)\n",
    "    assert isinstance(env.action_dim, int)\n",
    "    assert isinstance(env.if_discrete, bool)\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    assert state.shape == (env.state_dim,)\n",
    "\n",
    "    action = np.random.uniform(-1, +1, size=env.action_dim)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    assert isinstance(state, np.ndarray)\n",
    "    assert state.shape == (env.state_dim,)\n",
    "    assert isinstance(state, np.ndarray)\n",
    "    assert isinstance(reward, float)\n",
    "    assert isinstance(terminated, bool)\n",
    "    assert isinstance(info, dict) or (info is None)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_pendulum_env()\n",
    "    print('| Finish checking.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置解析\n",
    "\n",
    "环境、智能体、智能体交互过程中探索环境、训练的超参数，和设备有关的存储目录、计算设备等超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, agent_class = None, env_class = None, env_args = None) -> None:\n",
    "        self.agent_class = agent_class # agent = agent_class(...)\n",
    "        self.if_off_policy = self.get_if_off_policy() # 是否是off-policy\n",
    "\n",
    "        self.env_class = env_class \n",
    "        self.env_args = env_args\n",
    "        if self.env_args is None:\n",
    "            env_args = {'id': None, 'state_dim': None, 'action_dim': None, 'if_discrete': None}\n",
    "        self.id = env_args['id'] # 环境名称\n",
    "        self.state_dim = env_args['state_dim'] # 观测空间维度\n",
    "        self.action_dim = env_args['action_dim'] # 动作空间维度\n",
    "        self.if_discrete = env_args['if_discrete'] # 动作空间是否是离散空间\n",
    "\n",
    "        self.gamma = 0.99 # 折扣因子\n",
    "        self.reward_scale = 1.0 # 控制奖励大小\n",
    "\n",
    "        self.net_dims = (64,32) # 采用神经网络的维度：输入x输出\n",
    "        self.learning_rate = 6e-5 \n",
    "        self.soft_update_tau = 5e-3 \n",
    "        if self.if_off_policy: \n",
    "            self.batch_size = int(64) # mini-batch的大小\n",
    "            self.horizen_len = int(512) # 在每一个Episode中，智能体与环境互动的步数\n",
    "            self.buffer_size = int(1e6) # FIFO，replaybuffer的大小\n",
    "            self.repeat_times = 1.0 # 重复更新replaybuffer的次数\n",
    "        else:\n",
    "            self.batch_size = int(128)\n",
    "            self.horizen_len = int(2000) \n",
    "            self.buffer_size = None\n",
    "            self.repeat_times = 8.0\n",
    "\n",
    "        self.gpu_id = 0\n",
    "        self.thread_num = int(8) # 使用cpu数量\n",
    "        self.random_seed = int(0) # 初始化随机种子\n",
    "\n",
    "        self.cwd = None # 保存模型的当前工作目录\n",
    "        self.is_remove = True # 是否移除cwd目录\n",
    "        self.break_step = +np.inf # 如果神经网络的总训练次数超过该值则停止训练\n",
    "\n",
    "        self.eval_times = int(32)\n",
    "        self.eval_per_step = int(2e4)\n",
    "\n",
    "    def init_before_training(self):\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        torch.set_num_threads(self.thread_num)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "\n",
    "        if self.cwd is None:\n",
    "            self.cwd = f'./{self.id}_{self.agent_class.__name__[5:]}_{self.random_seed}'\n",
    "\n",
    "        if self.is_remove is None:\n",
    "            self.if_remove = bool(input(f\"| Arguments PRESS 'y' to REMOVE: {self.cwd}? \") == 'y')\n",
    "        if self.if_remove:\n",
    "            import shutil\n",
    "            shutil.rmtree(self.cwd, ignore_errors=True)\n",
    "            print(f\"| Arguments Remove cwd: {self.cwd}\")\n",
    "        else:\n",
    "            print(f\"| Arguments Keep cwd: {self.cwd}\")\n",
    "        \n",
    "        os.makedirs(self.cwd, exist_ok=True)\n",
    "\n",
    "    def get_if_off_policy(self) -> bool:\n",
    "        agent_name = self.agent_class.__name__ if self.agent_class else ''\n",
    "        on_policy_names = ('SARSA', 'VPG', 'A2C', 'A3C', 'TRPO', 'PPO', 'MPO')\n",
    "        return all([agent_name.find(s) == -1 for s in on_policy_names])\n",
    "    \n",
    "def get_gym_env_args(env, if_print: bool) -> dict:\n",
    "    \"\"\"Get a dict ``env_args`` about a standard OpenAI gym env information.\n",
    "\n",
    "    param env: a standard OpenAI gym env\n",
    "    param if_print: [bool] print the dict about env information.\n",
    "    return: env_args [dict]\n",
    "\n",
    "    env_args = {\n",
    "        'id': id,       # [str] the environment name, such as XxxXxx-v0\n",
    "        'state_dim': state_dim,     # [int] the dimension of state\n",
    "        'action_dim': action_dim,   # [int] the dimension of action or the number of discrete action\n",
    "        'if_discrete': if_discrete, # [bool] action space is discrete or continuous\n",
    "    }\n",
    "    \"\"\"\n",
    "    if {'unwrapped', 'observation_space', 'action_space', 'spec'}.issubset(dir(env)):  # isinstance(env, gym.Env):\n",
    "        id = env.unwrapped.spec.id\n",
    "\n",
    "        state_shape = env.observation_space.shape\n",
    "        state_dim = state_shape[0] if len(state_shape) == 1 else state_shape  # sometimes state_dim is a list\n",
    "\n",
    "        if_discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "        if if_discrete:  # make sure it is discrete action space\n",
    "            action_dim = env.action_space.n\n",
    "        elif isinstance(env.action_space, gym.spaces.Box):  # make sure it is continuous action space\n",
    "            action_dim = env.action_space.shape[0]\n",
    "            if any(env.action_space.high - 1):\n",
    "                print('WARNING: env.action_space.high', env.action_space.high)\n",
    "            if any(env.action_space.low + 1):\n",
    "                print('WARNING: env.action_space.low', env.action_space.low)\n",
    "        else:\n",
    "            raise RuntimeError('\\n| Error in get_gym_env_info(). Please set these value manually:'\n",
    "                               '\\n  `state_dim=int; action_dim=int; if_discrete=bool;`'\n",
    "                               '\\n  And keep action_space in range (-1, 1).')\n",
    "    else:\n",
    "        id = env.id\n",
    "        state_dim = env.state_dim\n",
    "        action_dim = env.action_dim\n",
    "        if_discrete = env.if_discrete\n",
    "\n",
    "    env_args = {'id': id,\n",
    "                'state_dim': state_dim,\n",
    "                'action_dim': action_dim,\n",
    "                'if_discrete': if_discrete, }\n",
    "    if if_print:\n",
    "        env_args_str = repr(env_args).replace(',', f\",\\n{'':11}\")\n",
    "        print(f\"env_args = {env_args_str}\")\n",
    "    return env_args\n",
    "\n",
    "def kwargs_filter(function, kwargs: dict) -> dict:\n",
    "    import inspect\n",
    "    sign = inspect.signature(function).parameters.values()\n",
    "    sign = {val.name for val in sign}\n",
    "    common_args = sign.intersection(kwargs.keys())\n",
    "    return {key: kwargs[key] for key in common_args}  # filtered kwargs\n",
    "\n",
    "\n",
    "def build_env(env_class=None, env_args=None):\n",
    "    if env_class.__module__ == 'gym.make':\n",
    "        env = env_class(**kwargs_filter(env_class, env_args.copy()))\n",
    "    if env_class.__module__ == 'gym.envs.registration':  # special rule\n",
    "        gym.logger.set_level(40)  # Block warning\n",
    "        env = env_class(id=env_args['id'])\n",
    "    else:\n",
    "        env = env_class(**kwargs_filter(env_class, env_args.copy()))\n",
    "    for attr_str in ('id', 'state_dim', 'action_dim', 'if_discrete'):\n",
    "        setattr(env, attr_str, env_args[attr_str])\n",
    "    return env\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net\n",
    "\n",
    "Agent依赖Net作为状态价值估计或者策略估计\n",
    "\n",
    "DQN：1，DDPG：actor和critic两个，PPO：actor和critic两个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def build_mlp(dims: List[int]) -> nn.Sequential:\n",
    "    net_list = []\n",
    "    for i in range(len(dims) - 1):\n",
    "        net_list.extend([nn.Linear(dims[i], dims[i+1]), nn.ReLU()])\n",
    "    del net_list[-1] # 移去最后一层的激活层\n",
    "    return nn.Sequential(*net_list)\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, dims: List[int], state_dim: int, action_dim: int):\n",
    "        super(QNet, self).__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, action_dim])\n",
    "        self.explore_rate = None\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        return self.net(state)\n",
    "    \n",
    "    def get_action(self, state: Tensor) -> Tensor: \n",
    "        if self.explore_rate < torch.rand(1):\n",
    "            action = self.net(state).argmax(dim=1, keepdim=True)\n",
    "        else:\n",
    "            action = torch.randint(self.action_dim, size=(state.shape[0],1))\n",
    "        return action\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, dims: List[int], state_dim: int, action_dim: int):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, action_dim])\n",
    "        self.explore_noise_std = None\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        action = self.net(state)\n",
    "        return action.tanh()\n",
    "\n",
    "    def get_action(self, state: Tensor) -> Tensor:\n",
    "        action_avg = self.net(state).tanh() # 双曲正切，奇函数，值域(-1,1)\n",
    "        dist = Normal(action_avg, self.explore_noise_std) # 对torch中每个值生成正态分布\n",
    "        action = dist.sample()\n",
    "        return action.clamp(-1.0, 1.0) # 截断函数\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, dims: List[int], state_dim: int, action_dim: int):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = build_mlp(dims=[state_dim + action_dim, *dims, 1])\n",
    "\n",
    "    def forward(self, state: Tensor, action: Tensor):\n",
    "        return self.net(torch.cat((state, action), dim=1)) # Q值\n",
    "    \n",
    "class ActorPPO(nn.Module):\n",
    "    def __init__(self, dims: List[int], state_dim: int, action_dim: int):\n",
    "        super(ActorPPO, self).__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, action_dim])\n",
    "        self.action_std_log = nn.Parameter(torch.zeros((1, action_dim)), requires_grad=True)  # trainable parameter\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        return self.net(state).tanh()\n",
    "    \n",
    "    def get_action(self, state: Tensor) -> (Tensor, Tensor):\n",
    "        action_avg = self.net(state)\n",
    "        action_std = self.action_std_log.exp()\n",
    "\n",
    "        dist = Normal(action_avg, action_std)\n",
    "        action = dist.sample()\n",
    "        logprob = dist.log_prob(action).sum(1)\n",
    "        return action, logprob\n",
    "    \n",
    "    def get_logprob_entropy(self, state: Tensor, action: Tensor) -> (Tensor, Tensor):\n",
    "        action_avg = self.net(state)\n",
    "        action_std = self.action_std_log.exp()\n",
    "\n",
    "        dist = Normal(action_avg, action_std)\n",
    "        logprob = dist.log_prob(action).sum(1)\n",
    "        entropy = dist.entropy().sum(1)\n",
    "        return logprob, entropy\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_action_for_env(action: Tensor) -> Tensor:\n",
    "        return action.tanh()\n",
    "    \n",
    "class CriticPPO(nn.Module):\n",
    "    def __init__(self, dims: List[int], state_dim: int, _action_dim: int):\n",
    "        super(CriticPPO, self).__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, 1])\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        return self.net(state)  # advantage value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试Net类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Finish checking.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn\n",
    "\n",
    "def check_q_net(state_dim=4, action_dim=2, batch_size=3, net_dims=(64, 32), gpu_id=0):\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "    state = torch.rand(size=(batch_size, state_dim), dtype=torch.float32, device=device)\n",
    "\n",
    "    '''check for agent.AgentDQN'''\n",
    "    act = QNet(dims=net_dims, state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "    act.explore_rate = 0.1\n",
    "\n",
    "    '''check for run.get_rewards_and_steps'''\n",
    "    action = act(state=state)\n",
    "    assert isinstance(action, Tensor)\n",
    "    assert action.dtype in {torch.float}\n",
    "    assert action.shape == (batch_size, action_dim)\n",
    "\n",
    "    '''check for agent.AgentDQN.explore_env'''\n",
    "    action = act.get_action(state=state)\n",
    "    assert isinstance(action, Tensor)\n",
    "    assert action.dtype in {torch.int, torch.long}\n",
    "    assert action.shape == (batch_size, 1)\n",
    "\n",
    "\n",
    "def check_actor(state_dim=4, action_dim=2, batch_size=3, net_dims=(64, 32), gpu_id=0):\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "    state = torch.rand(size=(batch_size, state_dim), dtype=torch.float32, device=device)\n",
    "\n",
    "    '''check'''\n",
    "    act = Actor(dims=net_dims, state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "    act.explore_noise_std = 0.1  # standard deviation of exploration action noise\n",
    "\n",
    "    action = act(state=state)\n",
    "    assert isinstance(action, Tensor)\n",
    "    assert action.dtype in {torch.float}\n",
    "    assert action.shape == (batch_size, action_dim)\n",
    "    assert torch.any((-1.0 <= action) & (action <= +1.0))\n",
    "\n",
    "    action = act.get_action(state=state)\n",
    "    assert isinstance(action, Tensor)\n",
    "    assert action.dtype in {torch.float}\n",
    "    assert action.shape == (batch_size, action_dim)\n",
    "    assert torch.any((-1.0 <= action) & (action <= +1.0))\n",
    "\n",
    "\n",
    "def check_critic(state_dim=4, action_dim=2, batch_size=3, net_dims=(64, 32), gpu_id=0):\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "    state = torch.rand(size=(batch_size, state_dim), dtype=torch.float32, device=device)\n",
    "    action = torch.rand(size=(batch_size, action_dim), dtype=torch.float32, device=device)\n",
    "\n",
    "    '''check'''\n",
    "    cri = Critic(dims=net_dims, state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "\n",
    "    q = cri(state=state, action=action)\n",
    "    assert isinstance(q, Tensor)\n",
    "    assert q.dtype in {torch.float}\n",
    "    assert q.shape == (batch_size, 1)\n",
    "\n",
    "\n",
    "def check_actor_ppo(state_dim=4, action_dim=2, batch_size=3, net_dims=(64, 32), gpu_id=0):\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "    state = torch.rand(size=(batch_size, state_dim), dtype=torch.float32, device=device)\n",
    "\n",
    "    '''check'''\n",
    "    act = ActorPPO(dims=net_dims, state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "    assert isinstance(act.action_std_log, nn.Parameter)\n",
    "    assert act.action_std_log.requires_grad\n",
    "\n",
    "    action = act(state=state)\n",
    "    assert isinstance(action, Tensor)\n",
    "    assert action.dtype in {torch.float}\n",
    "    assert action.shape == (batch_size, action_dim)\n",
    "    action = act.convert_action_for_env(action)\n",
    "    assert torch.any((-1.0 <= action) & (action <= +1.0))\n",
    "\n",
    "    action, logprob = act.get_action(state=state)\n",
    "    assert isinstance(action, Tensor)\n",
    "    assert action.dtype in {torch.float}\n",
    "    assert action.shape == (batch_size, action_dim)\n",
    "    assert torch.any((-1.0 <= action) & (action <= +1.0))\n",
    "    assert isinstance(logprob, Tensor)\n",
    "    assert logprob.shape == (batch_size,)\n",
    "\n",
    "    action = torch.rand(size=(batch_size, action_dim), dtype=torch.float32, device=device)\n",
    "    logprob, entropy = act.get_logprob_entropy(state=state, action=action)\n",
    "    assert isinstance(logprob, Tensor)\n",
    "    assert logprob.shape == (batch_size,)\n",
    "    assert isinstance(entropy, Tensor)\n",
    "    assert entropy.shape == (batch_size,)\n",
    "\n",
    "\n",
    "def check_critic_ppo(state_dim=4, action_dim=2, batch_size=3, net_dims=(64, 32), gpu_id=0):\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "    state = torch.rand(size=(batch_size, state_dim), dtype=torch.float32, device=device)\n",
    "\n",
    "    '''check'''\n",
    "    cri = CriticPPO(dims=net_dims, state_dim=state_dim, _action_dim=action_dim).to(device)\n",
    "\n",
    "    q = cri(state=state)\n",
    "    assert isinstance(q, Tensor)\n",
    "    assert q.dtype in {torch.float}\n",
    "    assert q.shape == (batch_size, 1)\n",
    "\n",
    "\n",
    "def check_build_mlp():\n",
    "    net_dims = (64, 32)\n",
    "    net = build_mlp(dims=net_dims)\n",
    "    assert isinstance(net, nn.Sequential)\n",
    "    assert len(net) == 1 == len(net_dims) * 2 - 3\n",
    "\n",
    "    net_dims = (64, 32, 16)\n",
    "    net = build_mlp(dims=net_dims)\n",
    "    assert isinstance(net, nn.Sequential)\n",
    "    assert len(net) == 3 == len(net_dims) * 2 - 3\n",
    "\n",
    "    net_dims = (64, 32, 16, 8)\n",
    "    net = build_mlp(dims=net_dims)\n",
    "    assert isinstance(net, nn.Sequential)\n",
    "    assert len(net) == 5 == len(net_dims) * 2 - 3\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_q_net()\n",
    "    check_actor()\n",
    "    check_critic()\n",
    "    check_actor_ppo()\n",
    "    check_critic_ppo()\n",
    "    check_build_mlp()\n",
    "    print('| Finish checking.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "三个Agent，DQN，DDPG，PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "class AgentBase:\n",
    "    def __init__(self, net_dims: List[int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.gamma = args.gamma\n",
    "        self.batch_size = args.batch_size\n",
    "        self.repeat_times = args.repeat_times\n",
    "        self.reward_scale = args.reward_scale\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.if_off_policy = args.if_off_policy\n",
    "        self.soft_update_tau = args.soft_update_tau\n",
    "\n",
    "        self.last_state = None\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "        \n",
    "        act_class = getattr(self, \"act_class\", None)\n",
    "        cri_class = getattr(self, \"cri_class\", None)\n",
    "\n",
    "        self.act = self.act_target = act_class(net_dims, state_dim, action_dim).to(self.device)\n",
    "        self.cri = self.cri_target = cri_class(net_dims, state_dim, action_dim).to(self.device) if cri_class else self.act\n",
    "\n",
    "        self.act_optimizer = torch.optim.Adam(self.act.parameters(), self.learning_rate)\n",
    "        self.cri_optimizer = torch.optim.Adam(self.cri.parameters(), self.learning_rate) if cri_class else self.act_optimizer\n",
    "\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    @staticmethod\n",
    "    def optimizer_update(optimizer, objective: Tensor):\n",
    "        optimizer.zero_grad()\n",
    "        objective.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(target_net: torch.nn.Module, current_net: torch.nn.Module, tau: float):\n",
    "        # 假定目标网络不是当前训练的网络\n",
    "        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n",
    "            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))\n",
    "\n",
    "class AgentDQN(AgentBase):\n",
    "    def __init__(self, net_dims: List[int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.act_class = getattr(self, \"act_class\", QNet)\n",
    "        self.cri_class = getattr(self, \"cri_class\", None) # DQN只有一个网络\n",
    "        super().__init__(net_dims, state_dim, action_dim, gpu_id, args)\n",
    "        self.act_target = deepcopy(self.act)\n",
    "        self.cri_target = deepcopy(self.cri)\n",
    "\n",
    "        self.act.explore_rate = getattr(args, \"explore_rate\", 0.25) \n",
    "        # epsilon贪婪算法的探索概率\n",
    "\n",
    "    def explore_env(self, env, horizon_len: int, if_random: bool = False) -> [Tensor]:\n",
    "        states = torch.zeros((horizon_len, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len, self.action_dim), dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        terminateds = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        truncateds = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        ary_state = self.last_state\n",
    "\n",
    "        get_action = self.act.get_action\n",
    "        for i in range(horizon_len):\n",
    "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device)\n",
    "            if if_random:\n",
    "                action = torch.randint(self.action_dim, size=(1,))[0]\n",
    "            else:\n",
    "                action = get_action(state.unsqueeze(0))[0,0]\n",
    "\n",
    "            ary_action = action.detach().cpu().numpy()\n",
    "            ary_state, reward, terminated, truncated, _ = env.step(ary_action)\n",
    "            if terminated or truncated:\n",
    "                ary_state, _ = env.reset()\n",
    "            \n",
    "            states[i] = state\n",
    "            actions[i] = action\n",
    "            rewards[i] = reward\n",
    "            terminateds[i] = terminated\n",
    "            truncateds[i] = truncated\n",
    "\n",
    "        self.last_state = ary_state\n",
    "        rewards = (rewards * self.reward_scale).unsqueeze(1)\n",
    "        undones = (1.0 - terminateds.type(torch.float32)) * (1.0 - truncateds.type(torch.float32)) \n",
    "        undones = undones.unsqueeze(1)\n",
    "        return states, actions, rewards, undones\n",
    "    \n",
    "    def update_net(self, buffer) -> [float]:\n",
    "        obj_critics = 0.0\n",
    "        q_values = 0.0\n",
    "\n",
    "        update_times = int(buffer.cur_size * self.repeat_times / self.batch_size)\n",
    "        assert update_times >= 1\n",
    "        for i in range(update_times):\n",
    "            obj_critic, q_value = self.get_obj_critic(buffer, self.batch_size)\n",
    "            self.optimizer_update(self.cri_optimizer, obj_critic)\n",
    "            self.soft_update(self.cri_target, self.cri, self.soft_update_tau)\n",
    "\n",
    "            obj_critics += obj_critic.item()\n",
    "            q_values += q_value.item()\n",
    "\n",
    "        return obj_critics / update_times, q_values / update_times\n",
    "    \n",
    "    def get_obj_critic(self, buffer, batch_size: int) -> (Tensor, Tensor):\n",
    "        with torch.no_grad():\n",
    "            state, action, reward, undone, next_state = buffer.sample(self.batch_size)\n",
    "            next_q = self.cri_target(next_state).max(dim=1, keepdim=True)[0]\n",
    "            q_label = reward + undone * self.gamma * next_q\n",
    "\n",
    "        q_value = self.cri(state).gather(1, action.long())\n",
    "        obj_critic = self.criterion(q_value, q_label)\n",
    "        return obj_critic, q_value.mean()\n",
    "    \n",
    "class AgentDDPG(AgentBase):\n",
    "    def __init__(self, net_dims: List[int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.act_class = getattr(self, 'act_class', Actor)\n",
    "        self.cri_class = getattr(self, 'cri_class', Critic)\n",
    "        super().__init__(net_dims, state_dim, action_dim, gpu_id, args)\n",
    "        self.act_target = deepcopy(self.act)\n",
    "        self.cri_target = deepcopy(self.cri)\n",
    "\n",
    "        self.act.explore_noise_std = getattr(args, 'explore_noise', 0.1)\n",
    "\n",
    "    def explore_env(self, env, horizon_len: int, if_random: bool = False) -> [Tensor]:\n",
    "        states = torch.zeros((horizon_len, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len, self.action_dim), dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        terminateds = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        truncateds = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        ary_state = self.last_state\n",
    "        get_action = self.act.get_action\n",
    "        for i in range(horizon_len):\n",
    "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device)\n",
    "            action = torch.rand(self.action_dim) * 2 - 1.0 if if_random else get_action(state.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "            ary_action = action.detach().cpu().numpy()\n",
    "            ary_state, reward, terminated, truncated, _ = env.step(ary_action)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                ary_state, _ = env.reset()\n",
    "\n",
    "            states[i] = state\n",
    "            actions[i] = action\n",
    "            rewards[i] = reward\n",
    "            terminateds[i] = terminated\n",
    "            truncateds[i] = truncated\n",
    "\n",
    "        self.last_state = ary_state\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        undones = (1.0 - terminateds.type(torch.float32)) * (1.0 - truncateds.type(torch.float32)) \n",
    "        undones = undones.unsqueeze(1)\n",
    "        return states, actions, rewards, undones\n",
    "    \n",
    "    def update_net(self, buffer) -> [float]:\n",
    "        obj_critics = obj_actors = 0.0\n",
    "        update_times = int(buffer.cur_size * self.repeat_times / self.batch_size)\n",
    "        assert update_times > 0\n",
    "        for i in range(update_times):\n",
    "            obj_critic, state = self.get_obj_critic(buffer, self.batch_size)\n",
    "            self.optimizer_update(self.cri_optimizer, obj_critic)\n",
    "            self.soft_update(self.cri_target, self.cri, self.soft_update_tau)\n",
    "            obj_critics += obj_critic.item()\n",
    "\n",
    "            action = self.act(state)\n",
    "            obj_actor = self.cri_target(state, action).mean()\n",
    "            self.optimizer_update(self.act_optimizer, -obj_actor)\n",
    "            self.soft_update(self.act_target, self.act, self.soft_update_tau)\n",
    "            obj_actors += obj_actor.item()\n",
    "\n",
    "        return obj_critics / update_times, obj_actors / update_times\n",
    "    \n",
    "    def get_obj_critic(self, buffer, batch_size: int) -> (Tensor, Tensor):\n",
    "        with torch.no_grad():\n",
    "            states, actions, rewards, undones, next_states = buffer.sample(batch_size)\n",
    "            next_actions = self.act_target(next_states)\n",
    "            next_q_values = self.cri_target(next_states, next_actions)\n",
    "            q_labels = rewards + undones * self.gamma * next_q_values\n",
    "\n",
    "        q_values = self.cri(states, actions)\n",
    "        obj_critic = self.criterion(q_values, q_labels)\n",
    "        return obj_critic, states\n",
    "    \n",
    "class AgentPPO(AgentBase):\n",
    "    def __init__(self, net_dims: List[int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.if_off_policy = False\n",
    "        self.act_class = getattr(self, \"act_class\", ActorPPO)\n",
    "        self.cri_class = getattr(self, \"cri_class\", CriticPPO)\n",
    "        super().__init__(net_dims, state_dim, action_dim, gpu_id, args)\n",
    "\n",
    "        self.ratio_clip = getattr(args, \"ratio_clip\", 0.25) # ratio.clamp(1 - clip, 1 + clip)\n",
    "        self.lambda_gae_adv = getattr(args, \"lambda_gae_adv\", 0.95) # 0.80~0.99\n",
    "        self.lambda_entropy = getattr(args, \"lambda_entropy\", 0.01) # 0.00~0.10\n",
    "        self.lambda_entropy = torch.tensor(self.lambda_entropy, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def explore_env(self, env, horizon_len: int) -> [Tensor]:\n",
    "        states = torch.zeros((horizon_len, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len, self.action_dim), dtype=torch.float32).to(self.device)\n",
    "        logprobs = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        terminateds = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        truncateds = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        ary_state = self.last_state\n",
    "\n",
    "        get_action = self.act.get_action\n",
    "        convert = self.act.convert_action_for_env\n",
    "        for i in range(horizon_len):\n",
    "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device)\n",
    "            action, logprob = [t.squeeze(0) for t in get_action(state.unsqueeze(0))[:2]]\n",
    "\n",
    "            ary_action = convert(action).detach().cpu().numpy()\n",
    "            ary_state, reward, terminated, truncated, _ = env.step(ary_action)\n",
    "            if terminated or truncated:\n",
    "                ary_state, _ = env.reset()\n",
    "\n",
    "            states[i] = state\n",
    "            actions[i] = action\n",
    "            logprobs[i] = logprob\n",
    "            rewards[i] = reward\n",
    "            terminateds[i] = terminated\n",
    "            truncateds[i] = truncated\n",
    "\n",
    "        self.last_state = ary_state\n",
    "        rewards = (rewards * self.reward_scale).unsqueeze(1)\n",
    "        undones = (1.0 - terminateds.type(torch.float32)) * (1.0 - truncateds.type(torch.float32)) \n",
    "        undones = undones.unsqueeze(1)\n",
    "        return states, actions, logprobs, rewards, undones\n",
    "    \n",
    "    def update_net(self, buffer) -> [float]:\n",
    "        with torch.no_grad():\n",
    "            states, actions, logprobs, rewards, undones = buffer\n",
    "            buffer_size = states.shape[0]\n",
    "\n",
    "            # 计算reward_sum\n",
    "            bs = 2 ** 10 # 当显存较小的时候用小的batchsize\n",
    "            values = [self.cri(states[i:i + bs]) for i in range(0, buffer_size, bs)]\n",
    "            values = torch.cat(values, dim=0).squeeze(1) # values.shape == (buffer_size, ..)\n",
    "            advantages = self.get_advantages(rewards, undones, values)\n",
    "            reward_sums = advantages + values \n",
    "            del rewards, values, undones\n",
    "\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std(dim=0) + 1e-5)\n",
    "        assert logprobs.shape == advantages.shape == reward_sums.shape == (buffer_size,)\n",
    "\n",
    "        obj_critics = 0.0\n",
    "        obj_actors = 0.0\n",
    "\n",
    "        update_times = int(buffer_size * self.repeat_times / self.batch_size)\n",
    "        assert update_times >= 1\n",
    "        for _ in range(update_times):\n",
    "            indices = torch.randint(buffer_size, size=(self.batch_size,), requires_grad=False)\n",
    "            state = states[indices]\n",
    "            action = actions[indices]\n",
    "            logprob = logprobs[indices]\n",
    "            advantage = advantages[indices]\n",
    "            reward_sum = reward_sums[indices]\n",
    "\n",
    "            value = self.cri(state).squeeze(1)\n",
    "            obj_critic = self.criterion(value, reward_sum)\n",
    "            self.optimizer_update(self.cri_optimizer, obj_critic)\n",
    "\n",
    "            new_logprob, obj_entropy = self.act.get_logprob_entropy(state, action)\n",
    "            ratio = (new_logprob - logprob.detach()).exp()\n",
    "            surrogate1 = advantage * ratio\n",
    "            surrogate2 = advantage * ratio.clamp(1 - self.ratio_clip, 1 + self.ratio_clip)\n",
    "            obj_surrogate = torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            obj_actor = obj_surrogate + obj_entropy.mean() * self.lambda_entropy\n",
    "            self.optimizer_update(self.act_optimizer, -obj_actor)\n",
    "\n",
    "            obj_actors += obj_actor.item()\n",
    "            obj_critics += obj_critic.item()\n",
    "\n",
    "        a_std_log = getattr(self.act, 'a_std_log', torch.zeros(1)).mean()\n",
    "        return obj_critics / update_times, obj_actors / update_times, a_std_log.item()\n",
    "    \n",
    "    def get_advantages(self, rewards: Tensor, undones: Tensor, values: Tensor) -> Tensor:\n",
    "        advantages = torch.empty_like(values)\n",
    "        \n",
    "        masks = undones * self.gamma\n",
    "        horizon_len = rewards.shape[0]\n",
    "\n",
    "        next_state = torch.tensor(self.last_state, dtype=torch.float32).to(self.device)\n",
    "        next_value = self.cri(next_state.unsqueeze(0)).detach().squeeze(1).squeeze(0)\n",
    "\n",
    "        advantage = 0\n",
    "        for t in range(horizon_len - 1, -1, -1):\n",
    "            delta = rewards[t] + masks[t] * next_value - values[t]\n",
    "            advantages[t] = advantage = delta + masks[t] * self.lambda_gae_adv * advantage\n",
    "            next_value = values[t] \n",
    "\n",
    "        return advantages\n",
    "    \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int, state_dim: int, action_dim: int, gpu_id: int = 0) -> None:\n",
    "        self.p = 0\n",
    "        self.if_full = False\n",
    "        self.cur_size = 0\n",
    "        self.max_size = max_size\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "\n",
    "        self.states = torch.empty((max_size, state_dim), dtype=torch.float32, device=self.device)\n",
    "        self.actions = torch.empty((max_size, action_dim), dtype=torch.float32, device=self.device)\n",
    "        self.rewards = torch.empty((max_size, 1), dtype=torch.float32, device=self.device)\n",
    "        self.undones = torch.empty((max_size, 1), dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def update(self, items: [Tensor]):\n",
    "        states, actions, rewards, undones = items\n",
    "        p = self.p + rewards.shape[0] # 指针移动\n",
    "        if p > self.max_size:\n",
    "            self.if_full = True\n",
    "            p0 = self.p\n",
    "            p1 = self.max_size\n",
    "            p2 = self.max_size - self.p\n",
    "            p = p - self.max_size\n",
    "\n",
    "            self.states[p0:p1], self.states[0:p] = states[:p2], states[-p:]\n",
    "            self.actions[p0:p1], self.actions[0:p] = actions[:p2], actions[-p:]\n",
    "            self.rewards[p0:p1], self.rewards[0:p] = rewards[:p2], rewards[-p:]\n",
    "            self.undones[p0:p1], self.undones[0:p] = undones[:p2], undones[-p:]\n",
    "        else:\n",
    "            self.states[self.p:p] = states\n",
    "            self.actions[self.p:p] = actions\n",
    "            self.rewards[self.p:p] = rewards\n",
    "            self.undones[self.p:p] = undones\n",
    "        self.p = p\n",
    "        self.cur_size = self.max_size if self.if_full else self.p\n",
    "\n",
    "    def sample(self, batch_size: int) -> [Tensor]:\n",
    "        ids = torch.randint(self.cur_size - 1, size=(batch_size,), requires_grad=False)\n",
    "        return self.states[ids], self.actions[ids], self.rewards[ids], self.undones[ids], self.states[ids + 1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Finish checking.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_agent_base(state_dim=4, action_dim=2, batch_size=3, net_dims=(64, 32), gpu_id=0):\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "    state = torch.rand(size=(batch_size, state_dim), dtype=torch.float32, device=device).detach()\n",
    "    action = torch.rand(size=(batch_size, action_dim), dtype=torch.float32, device=device).detach()\n",
    "\n",
    "    '''check AgentBase'''\n",
    "    agent = AgentDDPG(net_dims, state_dim, action_dim, gpu_id=gpu_id, args=Config())\n",
    "    AgentBase.__init__(agent, net_dims, state_dim, action_dim, gpu_id=gpu_id, args=Config())\n",
    "\n",
    "    '''check for run.render_agent'''\n",
    "    action_grad = agent.act(state)\n",
    "    q_value = agent.cri(state, action_grad)\n",
    "    obj_act = -q_value.mean()\n",
    "    assert agent.optimizer_update(agent.act_optimizer, obj_act) is None\n",
    "    q_value = agent.cri(state, action)\n",
    "    obj_cri = agent.criterion(q_value, torch.zeros_like(q_value).detach()).mean()\n",
    "    assert agent.optimizer_update(agent.cri_optimizer, obj_cri) is None\n",
    "\n",
    "    current_net = agent.cri\n",
    "    target_net = deepcopy(agent.cri)\n",
    "    assert agent.soft_update(target_net=target_net, current_net=current_net, tau=3e-5) is None\n",
    "\n",
    "\n",
    "def check_agent_dqn(batch_size=3, horizon_len=16, net_dims=[64, 32], gpu_id=0):\n",
    "    # env_args = {'id': 'CartPole-v1', 'state_dim': 4, 'action_dim': 2, 'if_discrete': True}\n",
    "    env_args = {'id': 'CartPole-v1', 'state_dim': 4, 'action_dim': 2, 'if_discrete': True}\n",
    "    env = build_env(env_class=gym.make, env_args=env_args)\n",
    "    state_dim = env_args['state_dim']\n",
    "    action_dim = env_args['action_dim']\n",
    "\n",
    "    '''init agent'''\n",
    "    buffer = ReplayBuffer(gpu_id=gpu_id, max_size=int(1e4), state_dim=state_dim, action_dim=action_dim)\n",
    "    args = Config()\n",
    "    args.batch_size = batch_size\n",
    "    agent = AgentDQN(net_dims=net_dims, state_dim=state_dim, action_dim=action_dim, gpu_id=gpu_id, args=args)\n",
    "    agent.last_state, _ = env.reset()\n",
    "\n",
    "    '''check for agent.explore_env'''\n",
    "    buffer_items = agent.explore_env(env=env, horizon_len=horizon_len, if_random=True)\n",
    "    buffer.update(buffer_items)\n",
    "    states, actions, rewards, undones = buffer_items\n",
    "    assert states.shape == (horizon_len, state_dim)\n",
    "    assert states.dtype in {torch.float, torch.int}\n",
    "    assert actions.shape == (horizon_len, 2)\n",
    "    assert actions.dtype in {torch.int, torch.long, torch.float32}\n",
    "    assert rewards.shape == (horizon_len, 1)\n",
    "    assert rewards.dtype == torch.float\n",
    "    assert undones.shape == (horizon_len, 1)\n",
    "    assert undones.dtype == torch.float  # undones is float, instead of int\n",
    "    assert set(undones.squeeze(1).cpu().data.tolist()).issubset({0.0, 1.0})  # undones in {0.0, 1.0}\n",
    "\n",
    "    buffer_items = agent.explore_env(env=env, horizon_len=horizon_len, if_random=False)\n",
    "    buffer.update(buffer_items)\n",
    "    states, actions, rewards, undones = buffer_items\n",
    "    assert states.shape == (horizon_len, state_dim)\n",
    "    assert states.dtype in {torch.float, torch.int}\n",
    "    assert actions.shape == (horizon_len, 2)\n",
    "    assert actions.dtype in {torch.int, torch.long, torch.float32}\n",
    "    assert rewards.shape == (horizon_len, 1)\n",
    "    assert rewards.dtype == torch.float\n",
    "    assert undones.shape == (horizon_len, 1)\n",
    "    assert undones.dtype == torch.float  # undones is float, instead of int\n",
    "    assert set(undones.squeeze(1).cpu().data.tolist()).issubset({0.0, 1.0})  # undones in {0.0, 1.0}\n",
    "\n",
    "    '''check for agent.update_net'''\n",
    "    buffer.update(buffer_items)\n",
    "    obj_critic, state = agent.get_obj_critic(buffer=buffer, batch_size=batch_size)\n",
    "    assert obj_critic.shape == ()\n",
    "    assert states.shape == (horizon_len, state_dim)\n",
    "    assert states.dtype in {torch.float, torch.int}\n",
    "\n",
    "    logging_tuple = agent.update_net(buffer=buffer)\n",
    "    assert isinstance(logging_tuple, tuple)\n",
    "    assert any([isinstance(item, float) for item in logging_tuple])\n",
    "    assert len(logging_tuple) >= 2\n",
    "\n",
    "\n",
    "def check_agent_ddpg(batch_size=3, horizon_len=16, net_dims=(64, 32), gpu_id=0):\n",
    "    env_args = {'id': 'Pendulum-v1', 'state_dim': 3, 'action_dim': 1, 'if_discrete': False}\n",
    "    env = build_env(env_class=gym.make, env_args=env_args)\n",
    "    state_dim = env_args['state_dim']\n",
    "    action_dim = env_args['action_dim']\n",
    "\n",
    "    '''init agent'''\n",
    "    buffer = ReplayBuffer(gpu_id=gpu_id, max_size=int(1e4), state_dim=state_dim, action_dim=action_dim, )\n",
    "    args = Config()\n",
    "    args.batch_size = batch_size\n",
    "    agent = AgentDDPG(net_dims=net_dims, state_dim=state_dim, action_dim=action_dim, gpu_id=gpu_id, args=args)\n",
    "    agent.last_state, _ = env.reset()\n",
    "\n",
    "    '''check for agent.explore_env'''\n",
    "    buffer_items = agent.explore_env(env=env, horizon_len=horizon_len, if_random=True)\n",
    "    states, actions, rewards, undones = buffer_items\n",
    "    assert states.shape == (horizon_len, state_dim)\n",
    "    assert states.dtype in {torch.float, torch.int}\n",
    "    assert actions.shape == (horizon_len, action_dim)\n",
    "    assert actions.dtype == torch.float\n",
    "    assert rewards.shape == (horizon_len, 1)\n",
    "    assert rewards.dtype == torch.float\n",
    "    assert undones.shape == (horizon_len, 1)\n",
    "    assert undones.dtype == torch.float  # undones is float, instead of int\n",
    "    assert set(undones.squeeze(1).cpu().data.tolist()).issubset({0.0, 1.0})  # undones in {0.0, 1.0}\n",
    "\n",
    "    buffer_items = agent.explore_env(env=env, horizon_len=horizon_len, if_random=False)\n",
    "    states, actions, rewards, undones = buffer_items\n",
    "    assert states.shape == (horizon_len, state_dim)\n",
    "    assert states.dtype in {torch.float, torch.int}\n",
    "    assert actions.shape == (horizon_len, action_dim)\n",
    "    assert actions.dtype == torch.float\n",
    "    assert rewards.shape == (horizon_len, 1)\n",
    "    assert rewards.dtype == torch.float\n",
    "    assert undones.shape == (horizon_len, 1)\n",
    "    assert undones.dtype == torch.float  # undones is float, instead of int\n",
    "    assert set(undones.squeeze(1).cpu().data.tolist()).issubset({0.0, 1.0})  # undones in {0.0, 1.0}\n",
    "\n",
    "    '''check for agent.update_net'''\n",
    "    buffer.update(buffer_items)\n",
    "    obj_critic, state = agent.get_obj_critic(buffer=buffer, batch_size=batch_size)\n",
    "    assert obj_critic.shape == ()\n",
    "    assert states.shape == (horizon_len, state_dim)\n",
    "    assert states.dtype in {torch.float, torch.int}\n",
    "\n",
    "    logging_tuple = agent.update_net(buffer=buffer)\n",
    "    assert isinstance(logging_tuple, tuple)\n",
    "    assert any([isinstance(item, float) for item in logging_tuple])\n",
    "    assert len(logging_tuple) >= 2\n",
    "\n",
    "\n",
    "def check_agent_ppo(batch_size=3, horizon_len=16, net_dims=(64, 32), gpu_id=0):\n",
    "    env_args = {'id': 'Pendulum-v1', 'state_dim': 3, 'action_dim': 3, 'if_discrete': False}\n",
    "    env = build_env(env_class=gym.make, env_args=env_args)\n",
    "    state_dim = env_args['state_dim']\n",
    "    action_dim = env_args['action_dim']\n",
    "\n",
    "    '''init agent'''\n",
    "    args = Config()\n",
    "    args.batch_size = batch_size\n",
    "    agent = AgentPPO(net_dims=net_dims, state_dim=state_dim, action_dim=action_dim, gpu_id=gpu_id, args=args)\n",
    "    agent.last_state, _ = env.reset()\n",
    "\n",
    "    convert = agent.act.convert_action_for_env\n",
    "    action = torch.rand(size=(batch_size, action_dim), dtype=torch.float32).detach() * 6 - 3\n",
    "    assert torch.any((action < -1.0) | (+1.0 < action))\n",
    "    action = convert(action)\n",
    "    assert torch.any((-1.0 <= action) & (action <= +1.0))\n",
    "\n",
    "    '''check for agent.explore_env'''\n",
    "    buffer_items = agent.explore_env(env=env, horizon_len=horizon_len)\n",
    "    states, actions, logprobs, rewards, undones = buffer_items\n",
    "    assert states.shape == (horizon_len, state_dim)\n",
    "    assert states.dtype in {torch.float, torch.int}\n",
    "    assert actions.shape == (horizon_len, action_dim)\n",
    "    assert actions.dtype == torch.float\n",
    "    assert logprobs.shape == (horizon_len,)\n",
    "    assert logprobs.dtype == torch.float\n",
    "    assert rewards.shape == (horizon_len, 1)\n",
    "    assert rewards.dtype == torch.float\n",
    "    assert undones.shape == (horizon_len, 1)\n",
    "    assert undones.dtype == torch.float  # undones is float, instead of int\n",
    "    assert set(undones.squeeze(1).cpu().data.tolist()).issubset({0.0, 1.0})  # undones in {0.0, 1.0}\n",
    "\n",
    "    '''check for agent.update_net'''\n",
    "    values = agent.cri(states).squeeze(1)\n",
    "    assert values.shape == (horizon_len,)\n",
    "    advantages = agent.get_advantages(rewards=rewards, undones=undones, values=values)\n",
    "    assert advantages.shape == (horizon_len,)\n",
    "    assert advantages.dtype in {torch.float, torch.int}\n",
    "\n",
    "    logging_tuple = agent.update_net(buffer=buffer_items)\n",
    "    assert isinstance(logging_tuple, tuple)\n",
    "    assert any([isinstance(item, float) for item in logging_tuple])\n",
    "    assert len(logging_tuple) >= 2\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_agent_base()\n",
    "    check_agent_dqn()\n",
    "    check_agent_ddpg()\n",
    "    check_agent_ppo()\n",
    "    print('| Finish checking.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Arguments Keep cwd: ./Pendulum_PPO_0\n",
      "| Arguments Keep cwd: ./Pendulum_PPO_0\n",
      "| Arguments Remove cwd: ./Pendulum_PPO_0\n",
      "| Arguments Remove cwd: ./Pendulum_PPO_0\n",
      "| Arguments Keep cwd: ./Pendulum_DDPG_0\n",
      "| Arguments Keep cwd: ./Pendulum-v1_DDPG_0\n",
      "Print_input(): input_str\n",
      "env_args = {'id': 'CartPole-v1',\n",
      "            'state_dim': 4,\n",
      "            'action_dim': 2,\n",
      "            'if_discrete': True}\n",
      "<class 'numpy.int64'>\n",
      "WARNING: env.action_space.high [2.]\n",
      "WARNING: env.action_space.low [-2.]\n",
      "env_args = {'id': 'Pendulum-v1',\n",
      "            'state_dim': 3,\n",
      "            'action_dim': 1,\n",
      "            'if_discrete': False}\n",
      "| Finish checking.\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "\n",
    "def check_config():\n",
    "    args = Config()  # check dummy Config\n",
    "    assert args.get_if_off_policy() is True\n",
    "\n",
    "    env_args = {'id': 'CartPole-v1', 'state_dim': 4, 'action_dim': 2, 'if_discrete': True}\n",
    "    env_class = gym.make\n",
    "    args = Config(agent_class=AgentDQN, env_class=env_class, env_args=env_args)\n",
    "    assert args.get_if_off_policy() is True\n",
    "\n",
    "    env_args = {'id': 'Pendulum', 'state_dim': 3, 'action_dim': 1, 'if_discrete': False}\n",
    "    env_class = PendulumEnv\n",
    "    args = Config(agent_class=AgentDDPG, env_class=env_class, env_args=env_args)\n",
    "    assert args.get_if_off_policy() is True\n",
    "\n",
    "    env_args = {'id': 'Pendulum', 'state_dim': 3, 'action_dim': 1, 'if_discrete': False}\n",
    "    env_class = PendulumEnv\n",
    "    args = Config(agent_class=AgentPPO, env_class=env_class, env_args=env_args)\n",
    "    assert args.get_if_off_policy() is False\n",
    "\n",
    "    args.if_remove = False\n",
    "    args.init_before_training()  # os.path.exists(args.cwd) == False\n",
    "    args.init_before_training()  # os.path.exists(args.cwd) == True\n",
    "    assert os.path.exists(args.cwd)\n",
    "    os.rmdir(args.cwd)\n",
    "\n",
    "    args.if_remove = True\n",
    "    args.init_before_training()  # os.path.exists(args.cwd) == False\n",
    "    args.init_before_training()  # os.path.exists(args.cwd) == True\n",
    "    assert os.path.exists(args.cwd)\n",
    "    os.rmdir(args.cwd)\n",
    "\n",
    "\n",
    "@patch('builtins.input', lambda *args: 'y')\n",
    "def check_config_init_before_training_yes():\n",
    "    env_args = {'id': 'Pendulum-v1', 'state_dim': 3, 'action_dim': 1, 'if_discrete': False}\n",
    "    env_class = gym.make\n",
    "    args = Config(agent_class=AgentDDPG, env_class=env_class, env_args=env_args)\n",
    "    args.if_remove = None\n",
    "    args.init_before_training()\n",
    "    assert os.path.exists(args.cwd)\n",
    "    os.rmdir(args.cwd)\n",
    "\n",
    "\n",
    "@patch('builtins.input', lambda *args: 'n')\n",
    "def check_config_init_before_training_no():\n",
    "    env_args = {'id': 'Pendulum', 'state_dim': 3, 'action_dim': 1, 'if_discrete': False}\n",
    "    env_class = PendulumEnv\n",
    "    args = Config(agent_class=AgentDDPG, env_class=env_class, env_args=env_args)\n",
    "    args.if_remove = None\n",
    "    args.init_before_training()\n",
    "    assert os.path.exists(args.cwd)\n",
    "    os.rmdir(args.cwd)\n",
    "\n",
    "\n",
    "@patch('builtins.input', lambda *args: 'input_str')\n",
    "def tutorial_unittest_mock_patch():\n",
    "    print('Print_input():', input())\n",
    "\n",
    "\n",
    "def check_kwargs_filter():\n",
    "    env_args = {'id': 'Pendulum-v1', 'state_dim': 3, 'action_dim': 1, 'if_discrete': False}\n",
    "    env_class = PendulumEnv\n",
    "    env = env_class(**kwargs_filter(env_class.__init__, env_args.copy()))\n",
    "    assert hasattr(env, 'reset')\n",
    "    assert hasattr(env, 'step')\n",
    "\n",
    "\n",
    "def check_build_env():\n",
    "    env_args = {'id': 'CartPole-v1', 'state_dim': 4, 'action_dim': 2, 'if_discrete': True}\n",
    "    env_class = gym.make\n",
    "    env = build_env(env_class=env_class, env_args=env_args)\n",
    "    assert isinstance(env.id, str)\n",
    "    assert isinstance(env.state_dim, int)\n",
    "    assert isinstance(env.action_dim, int)\n",
    "    assert isinstance(env.if_discrete, bool)\n",
    "\n",
    "    env_args = {'id': 'Pendulum-v1', 'state_dim': 3, 'action_dim': 1, 'if_discrete': False}\n",
    "    env_class = PendulumEnv\n",
    "    env = build_env(env_class=env_class, env_args=env_args)\n",
    "    assert isinstance(env.id, str)\n",
    "    assert isinstance(env.state_dim, int)\n",
    "    assert isinstance(env.action_dim, int)\n",
    "    assert isinstance(env.if_discrete, bool)\n",
    "\n",
    "\n",
    "def check_get_gym_env_args():\n",
    "    env_args = {'id': 'CartPole-v1', 'state_dim': 4, 'action_dim': 2, 'if_discrete': True}\n",
    "    env_class = gym.make\n",
    "    env = build_env(env_class=env_class, env_args=env_args)\n",
    "    env_args = get_gym_env_args(env, if_print=True)\n",
    "    assert isinstance(env_args['id'], str)\n",
    "    assert isinstance(env_args['state_dim'], int)\n",
    "    print(type(env_args['action_dim']))\n",
    "    assert isinstance(env_args['action_dim'], np.int64)\n",
    "    assert isinstance(env_args['if_discrete'], bool)\n",
    "\n",
    "    env_args = {'id': 'Pendulum-v1', 'state_dim': 3, 'action_dim': 1, 'if_discrete': False}\n",
    "    env_class = PendulumEnv\n",
    "    env = build_env(env_class=env_class, env_args=env_args)\n",
    "    env_args = get_gym_env_args(env, if_print=True)\n",
    "    assert isinstance(env_args['id'], str)\n",
    "    assert isinstance(env_args['state_dim'], int)\n",
    "    assert isinstance(env_args['action_dim'], int)\n",
    "    assert isinstance(env_args['if_discrete'], bool)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_config()\n",
    "    check_config_init_before_training_no()\n",
    "    check_config_init_before_training_yes()\n",
    "    tutorial_unittest_mock_patch()\n",
    "\n",
    "    check_kwargs_filter()\n",
    "    check_build_env()\n",
    "    check_get_gym_env_args()\n",
    "    print('| Finish checking.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "智能体与环境交互过程的控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, eval_env, eval_per_step: int = 1e4, eval_times: int = 8, cwd: str = '.') -> None:\n",
    "        self.cwd = cwd\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_step = 0\n",
    "        self.total_step = 0\n",
    "        self.start_time = time.time()\n",
    "        self.eval_times = eval_times # 获得episodic return的次数\n",
    "        self.eval_per_step = eval_per_step # 每步训练的评估次数\n",
    "\n",
    "        self.recorder = []\n",
    "        print(\"| Evaluator:\"\n",
    "              \"\\n| `step`: Number of samples, or total training steps, or running times of `env.step()`.\"\n",
    "              \"\\n| `time`: Time spent from the start of training to this moment.\"\n",
    "              \"\\n| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              \"\\n| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              \"\\n| `avgS`: Average of steps in an episode.\"\n",
    "              \"\\n| `objC`: Objective of Critic network. Or call it loss function of critic network.\"\n",
    "              \"\\n| `objA`: Objective of Actor network. It is the average Q value of the critic network.\"\n",
    "              f\"\\n| {'step':>8}  {'time':>8}  | {'avgR':>8}  {'stdR':>6}  {'avgS':>6}  | {'objC':>8}  {'objA':>8}\")\n",
    "        \n",
    "    def evaluate_and_save(self, actor, horizon_len: int, logging_tuple: tuple):\n",
    "        self.total_step += horizon_len\n",
    "        if self.eval_step + self.eval_per_step > self.total_step:\n",
    "            return\n",
    "        self.eval_step = self.total_step\n",
    "\n",
    "        rewards_steps_ary = [get_rewards_and_steps(self.eval_env, actor) for _ in range(self.eval_times)]\n",
    "        rewards_steps_ary = np.array(rewards_steps_ary, dtype=np.float32)\n",
    "        avg_r = rewards_steps_ary[:, 0].mean()  # 累积奖励的平均值\n",
    "        std_r = rewards_steps_ary[:, 0].std()  # 累积奖励的标准差\n",
    "        avg_s = rewards_steps_ary[:, 1].mean()  # 每个episode的平均补偿\n",
    "\n",
    "        used_time = time.time() - self.start_time\n",
    "        self.recorder.append((self.total_step, used_time, avg_r))\n",
    "\n",
    "        save_path = f\"{self.cwd}/actor_{self.total_step:012.0f}_{used_time:08.0f}_{avg_r:08.2f}.pth\"\n",
    "        torch.save(actor.state_dict(), save_path)\n",
    "        print(f\"| {self.total_step:8.2e}  {used_time:8.0f}  \"\n",
    "              f\"| {avg_r:8.2f}  {std_r:6.2f}  {avg_s:6.0f}  \"\n",
    "              f\"| {logging_tuple[0]:8.2f}  {logging_tuple[1]:8.2f}\")\n",
    "        \n",
    "    def close(self):\n",
    "        np.save(f\"{self.cwd}/recorder.npy\", np.array(self.recorder))\n",
    "        draw_learning_curve_using_recorder(self.cwd)\n",
    "\n",
    "def get_rewards_and_steps(env, actor, if_render: bool = False) -> (float, int):  # cumulative_rewards and episode_steps\n",
    "    if_discrete = env.if_discrete\n",
    "    device = next(actor.parameters()).device  # net.parameters() is a Python generator.\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    episode_steps = 0\n",
    "    cumulative_returns = 0.0  # sum of rewards in an episode\n",
    "    for episode_steps in range(12345):\n",
    "        tensor_state = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        tensor_action = actor(tensor_state).argmax(dim=1) if if_discrete else actor(tensor_state)\n",
    "        action = tensor_action.detach().cpu().numpy()[0]  # not need detach(), because using torch.no_grad() outside\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        cumulative_returns += reward\n",
    "\n",
    "        if if_render:\n",
    "            env.render()\n",
    "            time.sleep(0.02)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    cumulative_returns = getattr(env, 'cumulative_returns', cumulative_returns)\n",
    "    return cumulative_returns, episode_steps + 1\n",
    "\n",
    "def draw_learning_curve_using_recorder(cwd: str):\n",
    "    recorder = np.load(f\"{cwd}/recorder.npy\")\n",
    "\n",
    "    import matplotlib as mpl\n",
    "    mpl.use('Agg')  # write  before `import matplotlib.pyplot as plt`. `plt.savefig()` without a running X server\n",
    "    import matplotlib.pyplot as plt\n",
    "    x_axis = recorder[:, 0]\n",
    "    y_axis = recorder[:, 2]\n",
    "    plt.plot(x_axis, y_axis)\n",
    "    plt.xlabel('#samples (Steps)')\n",
    "    plt.ylabel('#Rewards (Score)')\n",
    "    plt.grid()\n",
    "\n",
    "    file_path = f\"{cwd}/LearningCurve.jpg\"\n",
    "    # plt.show()  # if use `mpl.use('Agg')` to draw figures without GUI, then plt can't plt.show()\n",
    "    plt.savefig(file_path)\n",
    "    print(f\"| Save learning curve in {file_path}\")\n",
    "\n",
    "def train_agent(args: Config):\n",
    "    args.init_before_training()\n",
    "\n",
    "    env = build_env(args.env_class, args.env_args)\n",
    "    agent = args.agent_class(args.net_dims, args.state_dim, args.action_dim, gpu_id=args.gpu_id, args=args)\n",
    "    agent.last_state = env.reset()\n",
    "\n",
    "    evaluator = Evaluator(eval_env=build_env(args.env_class, args.env_args),\n",
    "                          eval_per_step=args.eval_per_step,\n",
    "                          eval_times=args.eval_times,\n",
    "                          cwd=args.cwd)\n",
    "    \n",
    "    if args.if_off_policy:\n",
    "        buffer = ReplayBuffer(gpu_id=args.gpu_id,\n",
    "                              max_size=args.buffer_size,\n",
    "                              state_dim=args.state_dim,\n",
    "                              action_dim=1 if args.if_discrete else args.action_dim, )\n",
    "        buffer_items = agent.explore_env(env, args.horizon_len * args.eval_times, if_random=True)\n",
    "        buffer.update(buffer_items)  # warm up for ReplayBuffer\n",
    "    else:\n",
    "        buffer = []\n",
    "\n",
    "    # 开始训练\n",
    "    cwd = args.cwd\n",
    "    break_step = args.break_step\n",
    "    horizon_len = args.horizon_len\n",
    "    if_off_policy = args.if_off_policy\n",
    "    del args\n",
    "\n",
    "    torch.set_grad_enabled(False)\n",
    "    while True:\n",
    "        buffer_items = agent.explore_env(env, horizon_len)\n",
    "        if if_off_policy:\n",
    "            buffer.update(buffer_items)\n",
    "        else:\n",
    "            buffer[:] = buffer_items\n",
    "\n",
    "        torch.set_grad_enabled(True)\n",
    "        logging_tuple = agent.update_net(buffer)\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        evaluator.evaluate_and_save(agent.act, horizon_len, logging_tuple)\n",
    "        if (evaluator.total_step > break_step) or os.path.exists(f\"{cwd}/stop\"):\n",
    "            break  # stop training when reach `break_step` or `mkdir cwd/stop`\n",
    "    evaluator.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Save learning curve in ./temp/LearningCurve.jpg\n",
      "| Evaluator:\n",
      "| `step`: Number of samples, or total training steps, or running times of `env.step()`.\n",
      "| `time`: Time spent from the start of training to this moment.\n",
      "| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `avgS`: Average of steps in an episode.\n",
      "| `objC`: Objective of Critic network. Or call it loss function of critic network.\n",
      "| `objA`: Objective of Actor network. It is the average Q value of the critic network.\n",
      "|     step      time  |     avgR    stdR    avgS  |     objC      objA\n",
      "| 1.02e+03         0  | -1451.29   17.97     200  |     0.10      0.20\n",
      "| 2.05e+03         0  | -1025.02  106.17     200  |     0.30      0.40\n",
      "| Save learning curve in ./temp/LearningCurve.jpg\n",
      "| Finish checking.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def check_get_rewards_and_steps(net_dims=(64, 32)):\n",
    "    pass\n",
    "\n",
    "    \"\"\"discrete env\"\"\"\n",
    "    env_args = {'id': 'CartPole-v1', 'state_dim': 4, 'action_dim': 2, 'if_discrete': True}\n",
    "    env_class = gym.make\n",
    "    env = build_env(env_class=env_class, env_args=env_args)\n",
    "\n",
    "    '''discrete env, on-policy'''\n",
    "    actor = QNet(dims=net_dims, state_dim=env.state_dim, action_dim=env.action_dim)\n",
    "    cumulative_returns, episode_steps = get_rewards_and_steps(env=env, actor=actor)\n",
    "    assert isinstance(cumulative_returns, float)\n",
    "    assert isinstance(episode_steps, int)\n",
    "    assert episode_steps >= 1\n",
    "\n",
    "    \"\"\"continuous env\"\"\"\n",
    "    env_args = {'id': 'Pendulum-v1', 'state_dim': 3, 'action_dim': 1, 'if_discrete': False}\n",
    "    env_class = PendulumEnv\n",
    "    env = build_env(env_class=env_class, env_args=env_args)\n",
    "\n",
    "    '''continuous env, off-policy'''\n",
    "    actor = Actor(dims=net_dims, state_dim=env.state_dim, action_dim=env.action_dim)\n",
    "    cumulative_returns, episode_steps = get_rewards_and_steps(env=env, actor=actor)\n",
    "    assert isinstance(cumulative_returns, float)\n",
    "    assert isinstance(episode_steps, int)\n",
    "    assert episode_steps >= 1\n",
    "\n",
    "    '''continuous env, on-policy'''\n",
    "    actor = ActorPPO(dims=net_dims, state_dim=env.state_dim, action_dim=env.action_dim)\n",
    "    cumulative_returns, episode_steps = get_rewards_and_steps(env=env, actor=actor)\n",
    "    assert isinstance(cumulative_returns, float)\n",
    "    assert isinstance(episode_steps, int)\n",
    "    assert episode_steps >= 1\n",
    "\n",
    "\n",
    "def check_draw_learning_curve_using_recorder(cwd='./temp'):\n",
    "    os.makedirs(cwd, exist_ok=True)\n",
    "    recorder_path = f\"{cwd}/recorder.npy\"\n",
    "    recorder_len = 8\n",
    "\n",
    "    recorder = np.zeros((recorder_len, 3), dtype=np.float32)\n",
    "    recorder[:, 0] = np.linspace(1, 100, num=recorder_len)  # total_step\n",
    "    recorder[:, 1] = np.linspace(1, 200, num=recorder_len)  # used_time\n",
    "    recorder[:, 2] = np.linspace(1, 300, num=recorder_len)  # average of cumulative rewards\n",
    "    np.save(recorder_path, recorder)\n",
    "    draw_learning_curve_using_recorder(cwd)\n",
    "    assert os.path.exists(f\"{cwd}/LearningCurve.jpg\")\n",
    "    shutil.rmtree(cwd)\n",
    "\n",
    "\n",
    "def check_evaluator(net_dims=(64, 32), horizon_len=1024, eval_per_step=16, eval_times=2, cwd='./temp'):\n",
    "    env_args = {'id': 'Pendulum-v1', 'state_dim': 3, 'action_dim': 1, 'if_discrete': False}\n",
    "    env_class = PendulumEnv\n",
    "    env = build_env(env_class, env_args)\n",
    "    actor = Actor(dims=net_dims, state_dim=env.state_dim, action_dim=env.action_dim)\n",
    "\n",
    "    os.makedirs(cwd, exist_ok=True)\n",
    "    evaluator = Evaluator(eval_env=env, eval_per_step=eval_per_step, eval_times=eval_times, cwd=cwd)\n",
    "    evaluator.evaluate_and_save(actor=actor, horizon_len=horizon_len, logging_tuple=(0.1, 0.2))\n",
    "    evaluator.evaluate_and_save(actor=actor, horizon_len=horizon_len, logging_tuple=(0.3, 0.4))\n",
    "    evaluator.close()\n",
    "    assert os.path.exists(f\"{evaluator.cwd}/recorder.npy\")\n",
    "    assert os.path.exists(f\"{evaluator.cwd}/LearningCurve.jpg\")\n",
    "    shutil.rmtree(cwd)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_draw_learning_curve_using_recorder()\n",
    "    check_get_rewards_and_steps()\n",
    "    check_evaluator()\n",
    "    print('| Finish checking.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
